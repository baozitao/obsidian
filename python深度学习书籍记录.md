1. 通常需要对原始数据进行大量预处理，以便将其转换为张量输入到神经网络中。
2. 对于二分类问题（两个输出类别），网络的最后一层应该是只有一个单元并使用 sigmoid
3. 激活的 Dense 层，网络输出应该是 0~1 范围内的标量，表示概率值。
4. 对于二分类问题的 sigmoid 标量输出，你应该使用 binary_crossentropy 损失函数。
5. 无论你的问题是什么，rmsprop 优化器通常都是足够好的选择。这一点你无须担心。
6. 训练神经网络主要围绕以下四个方面：
	- 层，多个层组合成网络（或模型）。
	- 输入数据和相应的目标。
	- 损失函数，即用于学习的反馈信号。
	- 优化器，决定学习过程如何进行。
1. **随着神经网络在训练数据上的表现越来越好，模型最终会过拟合，并在前所未见的数据上得到越来越差的结果**。一定要一直监控模型在训练集之外的数据上的性能。
2. 神经网络的基本数据结构是层。**层是一个数据处理模块，将一个或多个输入张量转换为一个或多个输出张量。** 有些层是无状态的，但大多数的层是有状态的，即层的权重。权重是利用随机梯度下降学到的一个或多个张量，其中包含网络的知识。
3. **你可以将表示空间的维度直观地理解为“网络学习内部表示时所拥有的自由度”**。**隐藏单元越多（即更高维的表示空间），网络越能够学到更加复杂的表示，但网络的计算代价也变得更大，而且可能会导致学到不好的模式**（这种模式会提高训练数据上的性能，但不会提高测试数据上的性能）。
4. 如果没有 relu 等激活函数（也叫非线性），Dense 层将只包含两个线性运算——点积和加法：这样 Dense 层就只能学习输入数据的线性变换（仿射变换）：这种假设空间非常有限，无法利用多个表示层的优势，因为多个线性层堆叠实现的仍是线性运算，添加层数并不会扩展假设空间。**为了得到更丰富的假设空间，从而充分利用多层表示的优势，你需要添加非线性或激活函数。** relu 是深度学习中最常用的激活函数，但还有许多其他函数可选，它们都有类似的奇怪名称，比如 prelu、elu 等。
5. 具有多个输出的神经网络可能具有多个损失函数（每个输出对应一个损失函数）。但是，梯度下降过程必须基于单个标量损失值。因此，对于具有多个损失函数的网络，需要将所有损失函数取平均，变为一个标量值。
6. **选择正确的目标函数对解决问题是非常重要的。** 网络的目的是使损失尽可能最小化，因此，如果目标函数与成功完成当前任务不完全相关，那么网络最终得到的结果可能会不符合你的预期。想象一下，利用 SGD 训练一个愚蠢而又无所不能的人工智能，给它一个蹩脚的目标函数：“将所有活着的人的平均幸福感最大化”。为了简化自己的工作，这个人工智能可能会选择杀死绝大多数人类，只留几个人并专注于这几个人的幸福——因为平均幸福感并不受人数的影响。这可能并不是你想要的结果！请记住，你构建的所有神经网络在降低损失函数时和上述的人工智能一样无情。因此，一定要明智地选择目标函数，否则你将会遇到意想不到的副作用。
7. **网络的拓扑结构定义了一个假设空间（hypothesis space）**。你可能还记得第 1 章里机器学习的定义：“在预先定义好的可能性空间中，利用反馈信号的指引来寻找输入数据的有用表示。”**选定了网络拓扑结构，意味着将可能性空间（假设空间）限定为一系列特定的张量运算，将输入数据映射为输出数据。** 然后，你需要为这些张量运算的权重张量找到一组合适的值。
8. **选择正确的网络架构更像是一门艺术而不是科学**。虽然有一些最佳实践和原则，但只有动手实践才能让你成为合格的神经网络架构师。后面几章将教你构建神经网络的详细原则，也会帮你建立直觉，明白对于特定问题哪些架构有用、哪些架构无用。
9. **深度学习的几何解释**：**神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。** 因此，你**可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现，** 对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色，将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，让纸球恢复平整就是机器学习的内容：**为复杂的、高度折叠的数据流形找到简洁的表示。** 现在你应该能够很好地理解，为什么深度学习特别擅长这一点：**它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程**。