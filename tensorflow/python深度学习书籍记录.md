1. 机器学习的目的是拟合的情况下，得到**可以泛化（generalize）的模型**，即在前所未见的数据上表现很好的模型，而过拟合则是核心难点。不幸的是，没有一个魔法公式能够确定最佳层数或每层的最佳大小。你必须评估一系列不同的网络架构（当然是在验证集上评估，而不是在测试集上），以便为数据找到最佳的模型大小。要找到合适的模型大小，一般的工作流程是开始时选择相对较少的层和参数，然后逐渐增加层的大小或增加新层，直到这种增加对验证损失的影响变得很小。
2. 有一类无法解决的问题你应该知道，那就是**非平稳问题**：
	1. 从业务上观察，平稳序列的均值和方差不随索引增加而增加
	2. 均值函数是常数，自相关函数是时间差的函数。
	3. 严平稳：多维联合概率密度/分布不随时间变化而变化
3. 网络学习到的内容会越来越多，比如最开始只学到了黑色白色，慢慢学到了颜色的位置分布，颜色走向弯曲与不弯曲，进而学到了走线起始结束，还学到了更多的噪点及手写数字中蹩脚写法等等更高维度的东西，诚然，数据之间关系太多，均从低纬度、低阶数开始学习，学习完成低纬度后，高维度的学习基于低纬度学习成果继续在高维度开展学习，从易到难，自然界不也是这样么？
4. 神经网络喜欢处理较小的输入值
5. 通常需要对原始数据进行大量预处理，以便将其转换为张量输入到神经网络中。
6. _神经网络_将_张量_作为_输入_，并产生_张量_作为输出
7. 对于二分类问题（两个输出类别），网络的最后一层应该是只有一个单元并使用 sigmoid
8. 激活的 Dense 层，网络输出应该是 0~1 范围内的标量，表示概率值。
9. 对于二分类问题的 sigmoid 标量输出，你应该使用 binary_crossentropy 损失函数。
10. 无论你的问题是什么，rmsprop 优化器通常都是足够好的选择。这一点你无须担心。
11. 训练神经网络主要围绕以下四个方面：
	- 层，多个层组合成网络（或模型）。
	- 输入数据和相应的目标。
	- 损失函数，即用于学习的反馈信号。
	- 优化器，决定学习过程如何进行。
12. **随着神经网络在训练数据上的表现越来越好，模型最终会过拟合，并在前所未见的数据上得到越来越差的结果**。一定要一直监控模型在训练集之外的数据上的性能。
13. 神经网络的基本数据结构是层。**层是一个数据处理模块，将一个或多个输入张量转换为一个或多个输出张量。** 有些层是无状态的，但大多数的层是有状态的，即层的权重。权重是利用随机梯度下降学到的一个或多个张量，其中包含网络的知识。
14. **你可以将表示空间的维度直观地理解为“网络学习内部表示时所拥有的自由度”**。**隐藏单元越多（即更高维的表示空间），网络越能够学到更加复杂的表示，但网络的计算代价也变得更大，而且可能会导致学到不好的模式**（这种模式会提高训练数据上的性能，但不会提高测试数据上的性能）。
15. 如果没有 relu 等激活函数（也叫非线性），Dense 层将只包含两个线性运算——点积和加法：这样 Dense 层就只能学习输入数据的线性变换（仿射变换）：这种假设空间非常有限，无法利用多个表示层的优势，因为多个线性层堆叠实现的仍是线性运算，添加层数并不会扩展假设空间。**为了得到更丰富的假设空间，从而充分利用多层表示的优势，你需要添加非线性或激活函数。** relu 是深度学习中最常用的激活函数，但还有许多其他函数可选，它们都有类似的奇怪名称，比如 prelu、elu 等。
16. 具有多个输出的神经网络可能具有多个损失函数（每个输出对应一个损失函数）。但是，梯度下降过程必须基于单个标量损失值。因此，对于具有多个损失函数的网络，需要将所有损失函数取平均，变为一个标量值。
17. **选择正确的目标函数对解决问题是非常重要的。** 网络的目的是使损失尽可能最小化，因此，如果目标函数与成功完成当前任务不完全相关，那么网络最终得到的结果可能会不符合你的预期。想象一下，利用 SGD 训练一个愚蠢而又无所不能的人工智能，给它一个蹩脚的目标函数：“将所有活着的人的平均幸福感最大化”。为了简化自己的工作，这个人工智能可能会选择杀死绝大多数人类，只留几个人并专注于这几个人的幸福——因为平均幸福感并不受人数的影响。这可能并不是你想要的结果！请记住，你构建的所有神经网络在降低损失函数时和上述的人工智能一样无情。因此，一定要明智地选择目标函数，否则你将会遇到意想不到的副作用。
18. **网络的拓扑结构定义了一个假设空间（hypothesis space）**。你可能还记得第 1 章里机器学习的定义：“在预先定义好的可能性空间中，利用反馈信号的指引来寻找输入数据的有用表示。”**选定了网络拓扑结构，意味着将可能性空间（假设空间）限定为一系列特定的张量运算，将输入数据映射为输出数据。** 然后，你需要为这些张量运算的权重张量找到一组合适的值。
19. **选择正确的网络架构更像是一门艺术而不是科学**。虽然有一些最佳实践和原则，但只有动手实践才能让你成为合格的神经网络架构师。后面几章将教你构建神经网络的详细原则，也会帮你建立直觉，明白对于特定问题哪些架构有用、哪些架构无用。
20. **深度学习的几何解释**：**神经网络完全由一系列张量运算组成，而这些张量运算都只是输入数据的几何变换。** 因此，你**可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换可以通过许多简单的步骤来实现，** 对于三维的情况，下面这个思维图像是很有用的。想象有两张彩纸：一张红色，一张蓝色，将其中一张纸放在另一张上。现在将两张纸一起揉成小球。这个皱巴巴的纸球就是你的输入数据，每张纸对应于分类问题中的一个类别。神经网络（或者任何机器学习模型）要做的就是找到可以让纸球恢复平整的变换，从而能够再次让两个类别明确可分。通过深度学习，这一过程可以用三维空间中一系列简单的变换来实现，比如你用手指对纸球做的变换，让纸球恢复平整就是机器学习的内容：**为复杂的、高度折叠的数据流形找到简洁的表示。** 现在你应该能够很好地理解，为什么深度学习特别擅长这一点：**它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程**。
21. 每个数据点只能属于多个分类中的一个分类，则称之为单标签、多分类（single-label, multiclass classification）问题。如果每个数据点可以划分到多个类别（主题），那它就是一个多标签、多分类（multilabel, multiclass classification）问题。
22. 多分类问题：
	- 如果要对 N 个类别的数据点进行分类，网络的最后一层应该是大小为 N 的 Dense 层。
	- 对于单标签、多分类问题，网络的最后一层应该使用 softmax 激活，这样可以输出在 N
	- 个输出类别上的概率分布。
	- **这种问题的损失函数几乎总是应该使用分类交叉熵**。它将网络输出的概率分布与目标的真实分布之间的距离最小化。
	- 处理多分类问题的标签有两种方法。
		- 通过分类编码（也叫 one-hot 编码）对标签进行编码，然后使用 categorical_crossentropy 作为损失函数。
		- 将标签编码为整数，然后使用 sparse_categorical_crossentropy 损失函数。
	- 如果你需要将数据划分到许多类别中，应该避免使用太小的中间层，以免在网络中造成信息瓶颈。
23. 在数据挖掘数据处理过程中，不同评价指标往往具有不同的量纲和量纲单位，这样的情况会影响到数据分析的结果，**为了消除指标之间的量纲影响，需要进行数据标准化处理，以解决数据指标之间的可比性**。**原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价**。
24. 归一化、中心化、标准化**都是线性变换，不会造成数据信息丢失**：
	1. **归一化**：
		1. １）把数据变成(０，１)或者（1,1）之间的小数。主要是为了数据处理方便提出来的，把数据映射到0～1范围之内处理，更加便捷快速。２）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。归一化是一种简化计算的方式，即将有量纲的表达式，经过变换，化为无量纲的表达式，成为纯量。 
	2. **标准化：** **（将数据按比例缩放，使之落入一个小的特定区间）** 在机器学习中，我们可能要处理不同种类的资料，例如，音讯和图片上的像素值，这些资料可能是高维度的，资料标准化后会使每个特征中的数值平均变为0(将每个特征的值都减掉原始资料中该特征的平均)、标准差变为1，这个方法被广泛的使用在许多机器学习算法中(例如：支持向量机、逻辑回归和类神经网络)。 
	3. **中心化：** 平均值为0，对标准差无要求
25. 回归问题：
	- 回归问题使用的损失函数与分类问题不同。回归常用的损失函数是均方误差（MSE）。
	- 同样，回归问题使用的评估指标也与分类问题不同。显而易见，精度的概念不适用于回归问题。常见的回归指标是平均绝对误差（MAE）。
	- 如果输入**数据的特征具有不同的取值范围，应该先进行预处理**，对每个特征单独进行缩放。
	- 如果**可用的数据很少，使用 K 折验证可以可靠地评估模型**。
	- 如果可用的训练数据很少，最好使用隐藏层较少（通常只有一到两个）的小型网络，以避免严重的过拟合。
26. 评估模型的重点是将数据划分为三个集合：**训练集、验证集和测试集。在训练数据上训练模型**，在**验证数据上评估模型**。一旦找到了最佳参数，就在测试数据上最后测试一次。你可能会问，为什么不是两个集合：一个训练集和一个测试集？在训练集上训练模型，然后在测试集上评估模型。这样简单得多！**原因在于开发模型时总是需要调节模型配置，比如选择层数或每层大小［这叫作模型的超参数（hyperparameter），以便与模型参数（即权重）区分开］。这个调节过程需要使用模型在验证数据上的性能作为反馈信号。这个调节过程本质上就是一种学习：在某个参数空间中寻找良好的模型配置。** 因此，如果基于模型在验证集上的性能来调节模型配置，会很快导致模型在验证集上过拟合，即使你并没有在验证集上直接训练模型也会如此。
27. 神经网络的所有输入和目标都必须是浮点数张量（在特定情况下可以是整数张量）。无论处理什么数据（声音、图像还是文本），都必须首先将其转换为张量，这一步叫作数据向量化（data vectorization）。
28. 一般来说，将取值相对较大的数据（比如多位整数，比网络权重的初始值大很多）或异质数据（heterogeneous data，比如数据的一个特征在 0~1 范围内，另一个特征在 100~200 范围内）输入到神经网络中是不安全的。这么做可能导致较大的梯度更新，进而导致网络无法收敛。为了让网络的学习变得更容易，输入数据应该具有以下特征
	- 取值较小：大部分值都应该在 0~1 范围内。
	- 同质性（homogenous）：所有特征的取值都应该在大致相同的范围内。
	此外，下面这种更严格的标准化方法也很常见，而且很有用，虽然不一定总是必需的（例如，对于数字分类问题就不需要这么做）。
	- 将每个特征分别标准化，使其平均值为 0。
	- 将每个特征分别标准化，使其标准差为 1。
27. **多数情况下，一个机器学习模型无法从完全任意的数据中进行学习**。
28. 给你一个时钟图片作为输入，让你给出时钟上的时间：
	1. 如果你选择用图像的原始像素作为输入数据，那么这个机器学习问题将非常困难。你需要用卷积神经网络来解决这个问题，而且还需要花费大量的计算资源来训练网络。
	2. 但如果你从更高的层次理解了这个问题（你知道人们怎么看时钟上的时间），那么可以为机器学习算法找到更好的输入特征，比如你可以编写 5 行 Python 脚本，找到时钟指针对应的黑色像素并输出每个指针尖的 (x, y) 坐标，这很简单。然后，一个简单的机器学习算法就可以学会这些坐标与时间的对应关系。
	3. 你还可以进一步思考：进行坐标变换，将 (x, y) 坐标转换为相对于图像中心的极坐标。这样输入就变成了每个时钟指针的角度 theta。现在的特征使问题变得非常简单，根本不需要机器学习，因为简单的舍入运算和字典查找就足以给出大致的时间。
	4. **这就是特征工程的本质：用更简单的方式表述问题，从而使问题变得更容易。它通常需要深入理解问题。**
29. 对于现代深度学习，大部分特征工程都是不需要的，因为神经网络能够从原始数据中自动提取有用的特征。这是否意味着，只要使用深度神经网络，就无须担心特征工程呢？并不是这样，原因有两点。
	1. 良好的特征仍然可以让你用更少的资源更优雅地解决问题。例如，使用卷积神经网络来读取钟面上的时间是非常可笑的。
	2. 良好的特征可以让你用更少的数据解决问题。深度学习模型自主学习特征的能力依赖于大量的训练数据。如果只有很少的样本，那么特征的信息价值就变得非常重要。
30. **为了防止模型从训练数据中学到错误或无关紧要的模式，最优解决方法是获取更多的训练数据**。模型的训练数据越多，泛化能力自然也越好。如果无法获取更多数据，次优解决方法是调节模型允许存储的信息量，或对模型允许存储的信息加以约束。如果一个网络只能记住几个模式，那么优化过程会迫使模型集中学习最重要的模式，这样更可能得到良好的泛化。
31. 模型参数相对于训练数据与输出数据之间关系维度越充裕，模型就尝试记住更高阶的关系，比如你需要的是3阶关系，实际它记住了8阶，这多出来的就是过拟合，这样过拟合的数据会给你今后的数据添油加醋，反而导致预测不准确，当模型参数较少，会出现欠拟合，导致需要的关系没记录到
32. 神经网络就是学习输入数据与输出数据之间的变换映射关系
33. 防止过拟合的最简单的方法就是减小模型大小，即减少模型中可学习参数的个数（这由层数和每层的单元个数决定）。在深度学习中，模型中可学习参数的个数通常被称为模型的容量（capacity）。直观上来看，参数更多的模型拥有更大的记忆容量（memorization capacity），因此能够在训练样本和目标之间轻松地学会完美的字典式映射，这种映射没有任何泛化能力。例如，拥有 500 000 个二进制参数的模型，能够轻松学会 MNIST 训练集中所有数字对应的类别——我们只需让 50 000 个数字每个都对应 10 个二进制参数。但这种模型对于新数字样本的分类毫无用处。始终牢记：**深度学习模型通常都很擅长拟合训练数据，但真正的挑战在于泛化，而不是拟合。**