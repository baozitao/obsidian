## 1.Normalization的背景

### 1.1.独立同分布与白化

- **独立同分布**，即**independent and identically distributed**，简称为 i.i.d.指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。
    
    - 独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力
        
- **白化**，即**whitening**，是一个重要的数据预处理步骤。白化最典型的方法就是PCA。白化一般包含两个目的：
    
    - （1）去除特征之间的相关性 —> 独立；
        
    - （2）使得所有特征具有相同的均值和方差—> 同分布。
        

### 1.2.深度学习中的 Internal Covariate Shift

#### 1.2.1.概念理解

##### 什么是Covariate Shift？

深度神经网络涉及到很多层的**叠加**，而每一层的**参数更新**会导致上层的**输入数据分布发生变化**，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新，导致深度神经网络模型的训练很困难。

Convariate，协变量，在实验的设计中，协变量是一个独立变量（解释变量），不为实验者所操纵，但仍影响实验结果

机器学习中有一个经典假设：源空间和目标空间的数据分布是一致的（训练集和测试集的分布是一致的）。而实际上模型的输入和输出的联合分布在训练数据和测试数据之间是不同的，这称为dataset shift。dataset shift的一种简明情况就是**Covariate Shift**（协变量变化）仅输入分布发生变化，而在给定输入的输出条件分布保持不变。也即，源空间和目标空间有概率是一致的，但是其边缘概率不同。

##### 什么是Internal Covariate Shift？

**Internal Covariate Shift**（内部协变量偏移），也叫ICS。可以理解为，在多层网络训练中，后面层次的[神经网络](https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020)接受到的节点受到前面层次的网络参数变化导致该层输入数据的分布发生了变化。泛指，在多层数据网络中各层之间参数变化引起的数据分布发生变化的现象。

#### 1.2.2.ICS导致的问题

简单来说，**就是使得每个神经元的输入数据不再是“独立同分布”了。**

其一，上层参数需要不断适应新的输入数据分布，降低学习速度。  

其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止，前一层学的太猛，导致后层不学了。  

其三，层与层之间学习成果不独立，每层的更新都会影响到其它层，因此每层的参数更新策略都要尽可能谨慎


## 2.Normalization的通用框架与基本思想

如何解决ICS的导致的输入数据不再满足独立同分布的问题？

理论上就是对每一层数据都做“白化”操作。基本思想：在将 x 送给神经元之前，先对其做平移和伸缩变换， 将 x 的分布规范化成在固定区间范围的标准分布

通用的变换框架如下所示：

![](https://img2022.cnblogs.com/blog/2513174/202209/2513174-20220928133114007-192558438.png)

（1） μ是平移参数（shift parameter）， σ是缩放参数（scale parameter）。通过这两个参数进行 shift 和 scale 变换：

![](https://img2022.cnblogs.com/blog/2513174/202209/2513174-20220928133114198-1369276338.png)

得到的数据符合均值为 0、方差为1的标准分布。

（2） b是再平移参数（re-shift parameter）， g是再缩放参数（re-scale parameter）。将 上一步得到的 ![](https://img2022.cnblogs.com/blog/2513174/202209/2513174-20220928133114156-206991235.png) 进一步变换为：![](https://img2022.cnblogs.com/blog/2513174/202209/2513174-20220928133114197-1502567206.png)  最终得到的数据符合均值为b、方差为g²的分布

- 第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围，因此我们要进行（2），保证模型的表达能力不会因为规范性而下降。
    
- **（2）中的参数都是可以学习的，这使得Normalization可以充分尊重底层的学习结果，还保证获取了非线性的表达能力**
    
    - 第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。
        
- （2）中的参数很容易通过梯度下降进行学习，简化神经网络的训练。