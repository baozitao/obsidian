1. 与其他所有神经网络一样，深度学习模型不会接收原始文本作为输入，它只能处理数值张量，文本向量化（vectorize）是指将文本转换为数值张量的过程。
	- 将文本分割为单词，并将每个单词转换为一个向量。
	- 将文本分割为字符，并将每个字符转换为一个向量。
	- 提取单词或字符的 n-gram，并将每个 n-gram 转换为一个向量。n-gram 是多个连续单词或字符的集合（n-gram 之间可重叠）。
1. **将文本分解而成的单元（单词、字符或 n-gram）叫作标记（token），将文本分解成标记的过程叫作分词（tokenization）**。**所有文本向量化过程都是应用某种分词方案**，然后**将数值向量与生成的标记相关联**。**这些向量组合成序列张量，被输入到深度神经网络中**（见图 6-1）。将向量与标记相关联的方法有很多种。本节将介绍两种主要方法：对标记做 one-hot 编码（one-hot encoding）与标记嵌入［token embedding，通常只用于单词，叫作词嵌入（word embedding）］。本节剩余内容将解释这些方法，并介绍如何使用这些方法，将原始文本转换为可以输入到 Keras网络中的 Numpy 张量。
2. 提取 n-gram 是一种特征工程，深度学习不需要这种死板而又不稳定的方法
3. **没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的**，比如常见的视觉特征或语义特征。在这种情况下，重复使用在其他问题上学到的特征，这种做法是有道理的。
4. **词嵌入的作用**应该是**将人类的语言映射到几何空间中**。例如，在一个合理的嵌入空间中，同义词应该被嵌入到相似的词向量中，一般来说，任意两个词向量之间的几何距离（比如 L2 距离）应该和这两个词的语义距离有关（表示不同事物的词被嵌入到相隔很远的点，而相关的词则更加靠近）。除了距离，你可能还希望嵌入空间中的特定方向也是有意义的。
5. 词嵌入为向量又两个办法：
	1. **在神经网络中加入词嵌入层，层初始向量随机，一边学习网络，一边学习词嵌入层**：要将一个词与一个密集向量相关联，最简单的方法就是随机选择向量。这种方法的问题在于，得到的嵌入空间没有任何结构。例如，accurate 和 exact 两个词的嵌入可能完全不同，尽管它们在大多数句子里都是可以互换的。深**度神经网络很难对这种杂乱的、非结构化的嵌入空间进行学习，词向量之间的几何关系应该表示这些词之间的语义关系**。
	2. 使用已经预训练的嵌入网络层，这些层已经结构化;
6. 有时可用的训练数据很少，以至于只用手头数据无法学习适合特定任务的词嵌入。那么应该怎么办？**你可以从预计算的嵌入空间中加载嵌入向量（你知道这个嵌入空间是高度结构化的，并且具有有用的属性，即抓住了语言结构的一般特点），而不是在解决问题的同时学习词嵌入。在自然语言处理中使用预训练的词嵌入，其背后的原理与在图像分类中使用预训练的卷积神经网络是一样的：没有足够的数据来自己学习真正强大的特征，但你需要的特征应该是非常通用的，比如常见的视觉特征或语义特征。在这种情况下，重复使用在其他问题上学到的特征，这种做法是有道理的。**
7. **这种词嵌入通常是利用词频统计计算得出的**（观察哪些词共同出现在句子或文档中），用到的技术很多，有些涉及神经网络，有些则不涉及
8. LSTM 层是 SimpleRNN 层的一种变体，它增加了一种携带信息跨越多个时间步的方法。假设有一条传送带，其运行方向平行于你所处理的序列。序列中的信息可以在任意位置跳上传送带，然后被传送到更晚的时间步，并在需要时原封不动地跳回来。这实际上就是 LSTM 的原理：**它保存信息以便后面使用，从而防止较早期的信号在处理过程中逐渐消失**。
9. 但归根结底，这些解释并没有多大意义，因为这些运算的**实际效果是由参数化权重决定的**，而权重是以端到端的方式进行学习，每次训练都要从头开始，**不可能为某个运算赋予特定的目的**。**RNN 单元的类型（如前所述）决定了你的假设空间**，即在训练期间搜索良好模型配置的空间，**但它不能决定 RNN 单元的作用**，**那是由单元权重来决定的**。同一个单元具有不同的权重，可以实现完全不同的作用。因此，**组成 RNN 单元的运算组合，最好被解释为对搜索的一组约束，而不是一种工程意义上的设计**。**对于研究人员来说，这种约束的选择（即如何实现 RNN 单元）似乎最好是留给最优化算法来完成**（比如遗传算法或强化学习过程），而不是让人类工程师来完成。在未来，那将是我们构建网络的方式。**总之，你不需要理解关于 LSTM 单元具体架构的任何内容。作为人类，理解它不应该是你要做的。你只需要记住 LSTM 单元的作用：允许过去的信息稍后重新进入，从而解决梯度消失问题。**