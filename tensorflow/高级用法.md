1. 到目前为止，本书介绍的所有神经网络都是用 Sequential 模型实现的。Sequential 模型假设，网络只有一个输入和一个输出，而且网络是层的线性堆叠这是一个经过普遍验证的假设。这种网络配置非常常见，以至于本书前面只用 Sequential 模型类就能够涵盖许多主题和实际应用。但有些情况下这种假设过于死板。有些网络需要多个独立的输入，有些网络则需要多个输出，而有些网络在层与层之间具有内部分支，**这使得网络看起来像是层构成的图（graph），而不是层的线性堆叠**。
2. 构建深度学习模型时，你必须做出许多看似随意的决定：应该堆叠多少层？每层应该、包含多少个单元或过滤器？激活应该使用 relu 还是其他函数？在某一层之后是否应该使用 BatchNormalization ？应该使用多大的 dropout 比率？还有很多。这些在架构层面的参数叫作超参数（hyperparameter），以便将其与模型参数区分开来，后者通过反向传播进行训练。
3. 如果你想要在某项任务上达到最佳性能，那么就不能满足于一个容易犯错的人随意做出的选择。即使你拥有很好的直觉，最初的选择也几乎不可能是最优的。你可以**手动调节你的选择、重新训练模型，如此不停重复来改进你的选择，这也是机器学习工程师和研究人员大部分时间都在做的事情**。但是，**整天调节超参数不应该是人类的工作，最好留给机器去做**。
4. 因此，你需要**制定一个原则，系统性地自动探索可能的决策空间**。你**需要搜索架构空间，并根据经验找到性能最佳的架构**。这正是**超参数自动优化领域的内容。这个领域是一个完整的研究领域，而且很重要**
5. 给定许多组超参数，使用验证性能的历史来选择下一组需要评估的超参数的算法。有多种不同的技术可供选择：贝叶斯优化、遗传算法、简单随机搜索等。**训练模型权重相对简单：在小批量数据上计算损失函数，然后用反向传播算法让权重向正确的方向移动。与此相反，更新超参数则非常具有挑战性。**
6. q
